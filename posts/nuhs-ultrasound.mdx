---
title: "Mixed Reality Ultrasound"
media: ""
color: "#FAFF00"
mask: "shape3.svg"
ratioW: 375
ratioH: 812
isFeatured: false
toc: true
w: 2
h: 2
x: 0
y: 0
---

<PostLayout>

# Mixed Reality Ultrasound

</PostLayout>

<div className="row d-flex justify-content-between">

<div className="col-md-4 pe-md-4">
  <p className="lesson-header">Ultrasound</p>
  <p>
    Ultrasound imaging is frequently-used, versatile real-time imaging tool to
    create images of inside the body.
  </p>
  <p>
    The two main use cases of ultrasound imaging are for <b>diagnosis</b>, to
    identify and monitor conditions and for <b>intervention</b>, to guide
    medical procedures such as biopsies and drainages.
  </p>
</div>

<div className="col-md-4 px-md-4">
  <p className="lesson-header">Vision</p>
  <p>
    Clinicians can rely on <b>surgical navigation assistance</b> that provides a
    natural sense of proximity and perspective, within actual view of working
    environment, to guide procedures.
  </p>
  <BlockQuote>
    Conventional training takes ten years. Digital twin technology could
    significantly reduce training time. <br />
    <br />
    <em className="text-muted small">
      Cardiothoracic Surgeon <br />
      National University Heart Centre, Singapore
    </em>
  </BlockQuote>
</div>

<div className="col-md-4 ps-md-4">
  <p className="lesson-header">Scope</p>
  <ol>
    <li>
      <b>Visualize 2D and 3D ultrasound data relative to patient's body.</b>{" "}
      Clinicians can perceive target area's actual physical location in
      patient's body using anatomical hologram.
    </li>
    <li>
      <b>Track of medical instruments in real-time.</b>
      Clinicians can understand how instrument is moving inside the patient through
      holographic instrument interacting with patient's anatomy hologram.
    </li>
  </ol>
</div>

</div>

<PostLayout>

## Pain points

<p className="lesson-header">Interpret 3D using 2D images</p>

1. <b>Difficult to approximate location.</b> It is difficult to sense depth within
   a structure using 2D images. 2D ultrasound imaging doesn’t capture depth information
   and it is prone to perspective distortion. It takes many years for clinicians
   in real patient settings to develop the skill to translate 2D and spatially conceptualize.{" "}
2. <b>Ergonomics</b> Ultrasound machines need to be pushed around in big trolleys
   and wires get in the way over the patient. Clinicians often have to look down
   to where they are scanning, and look up towards the machine to view the feed.

</PostLayout>

<div className="container-fluid margin-top-36">
  <div className="row d-flex">
    <div className="col-md-4 padding-right-16 padding-bottom-24">
      <MarkdownImage
        src="/images/work/nuhs/holotwin-mock.png"
        width="1096px"
        height="616.5px"
        layout="fill"
      />
      <figcaption>
        In procedures, the needle will be occluded beneath the skin. The virtual
        instrument will be rendered in occluded part of the patient, using
        holographic guidance.
      </figcaption>
    </div>
  </div>
</div>

<div className="container-fluid margin-top-36">
<h2 id="3d-ultrasound">3D Ultrasound</h2>
  <div className="row d-flex">
    <div className="col-md-4 padding-right-24">
    <p className="lesson-header">
      Composite 3D reconstruction of 2D ultrasound video frames
    </p>
    <p className="text-black-50 padding-top-16">End-to-end task flow for generating 3D ultrasound</p>
    <ol>
    <li>
      <b>Define scan area.</b> When the user launches app, the cuboid appears. The
      user drags and scales holographic cuboid to fit the target area on patient to
      scan.
    </li>
    <li>
      <b>Scan.</b> The user uses the ultrasound probe to scan the cuboid area, using
      the video feed and guidance that shows scanned area.
    </li>
    <li>
      <b>Generate 3D.</b> After scanning, the user generates 3D scan using the
      primary CTA, anchored to the guidance.
    </li>
    <li>
      <b>View 3D reconstruction.</b> The holographic true size 3D model appears
      inside the scan area bounding box, which is directly on the patient.
    </li>
  </ol>
</div>

<div className="col-md-8">
     <LoopingVideo src="/images/work/nuhs/3drecon.mp4" />
      <figcaption>Demo testing ultrasound scan of Lego bricks embedded in agar model</figcaption>
    </div>
</div>
</div>

<div className="container-fluid margin-top-36">
  <div className="row d-flex">
    <h2 id="nav">Navigation</h2>
    <div className="col-md-4 padding-right-24 padding-bottom-24">
      <p>
       At first, we developed the app menu that followed the{" "}
        <a href="https://learn.microsoft.com/en-us/windows/mixed-reality/design/hand-menu">
          Microsoft MRTK hand menu conventions
        </a>
        . This wasn’t an issue in other Microsoft HL2 apps such as aploQar’s VSI.</p>
        
        <p>However, we uncovered usability issues following the guidelines in our specific use case:
        <br/>
        <ol>
          <li>HL2 recognizes and interprets hands to use the hand menu. During usability testing, HL2 also mistook the patient's hands instead of the clinician's, opening the menu unintentionally.</li>
          <li>Both the clinician's hands might be occupied: ultrasound probe in one hand, medical instrument in the other.</li>
          <li>Left-handed clinicians find it more difficult to use the menu.</li>
        </ol>
      </p>
    </div>
    <div className="col-md-4 padding-right-16 padding-bottom-24">
      <LoopingVideo src="/images/work/nuhs/menu-default.mp4"/>
      <figcaption>
        Standard HoloLens 2 <a href="https://learn.microsoft.com/en-us/hololens/hololens2-basic-usage#start-gesture">Start gesture</a>. Open palm facing user and tap on inner wrist <i>Start</i> icon with other hand.
      </figcaption>
    </div>
    <div className="col-md-4 padding-left-16">
      <LoopingVideo src="/images/work/nuhs/menu-initial.mp4" />
      <figcaption>
        Initial design used the same approach, where the app menu was anchored to the user's hand.
      </figcaption>
    </div>

  </div>
</div>

<div className="container-fluid margin-top-36">
  <div className="row d-flex">
    <div className="col-md-4 padding-right-24">
      <p className="lesson-header">Head-locked</p>
      <LoopingVideo src="/images/work/nuhs/initialspawn.mp4" />
      <p className="padding-top-16">
        Instead of anchoring the menu to the user's left wrist, the menu was
        changed to always appear in the same position in the user's field of
        view. The menu rotates with the head so it feels natural as the user moves. The tilt range was defined so that the menu doesn't jerk with every small movement. 
      </p>
    </div>
    <div className="col-md-4 padding-right-12 padding-left-12 padding-bottom-24">
      <p className="lesson-header">Default position</p>
      <LoopingVideo src="/images/work/nuhs/startscan.mp4" />
      <p className="padding-top-16">
        Collapsed menu appears 30–34cm in front of the user.
        The convention is 45cm for direct manipulation, but we found that a shorter distance makes the menu more intuitively reachable, especially as more operations are layered in the user's view. 
      </p>
    </div>
    <div className="col-md-4 padding-left-24">
      <p className="lesson-header">Moveable position</p>
      <LoopingVideo src="/images/work/nuhs/draggable.mp4" />
      <p className="padding-top-16">
      The draggable menu is offset to the left by default, so the center view
        is unobstructed. Clinicians can drag the menu to suit their unique workflows and preferences, and then it will be locked in their field of view. 
      </p>
    </div>

  </div>
</div>

<div className="container-fluid margin-top-36">
  <div className="row d-flex">
    <h2 id="multimodal">Multimodal design</h2>
    <div className="col-md-4 padding-right-24">
      <p className="lesson-header">Psychophysics & human perception</p>
      <p>
        Conducting ultrasound on a patient requires the clinician to be
        comfortable and aware of their context. The holographic components must
        not distort the view or disorient the user through both sense of touch
        and sight.
      </p>
      <p>
         To create intuitive interactions of holographic elements, I
        experimented with positioning in 3D world space at every stage of the
        design process and worked closely with Engineering to fine-tune motion
        design.
      </p>
    </div>
    <div className="col-md-4 padding-right-16 padding-bottom-24">
      <LoopingVideo src="/images/work/nuhs/position-quickview.mp4" />
      <figcaption>
        Quick view: Default scene shows true-size and zoomed-in 2D ultrasound
        video feed
      </figcaption>
    </div>
    <div className="col-md-4 padding-left-16">
      <LoopingVideo src="/images/work/nuhs/position-3dscan.mp4" />
      <figcaption>
        Scanning for 3D reconstruction: Default scene shows scanning guidance
        and zoomed-in 2D ultrasound video feed
      </figcaption>
    </div>
  </div>
</div>

<div className="container-fluid margin-top-36">
  <div className="row d-flex">
    <div className="col-md-4 padding-right-24">
      <p className="lesson-header">2D in 3D world space</p>
      <LoopingVideo src="/images/work/nuhs/initialspawn.mp4" />
      <p className="padding-top-16">
        What's most different about designing mixed reality experiences is 2D
        elements need to be designed as 3D.
      </p>
    </div>
    <div className="col-md-4 padding-right-12 padding-left-12 padding-bottom-24">
      <p className="lesson-header">Vertical & transverse axes</p>
      <LoopingVideo src="/images/work/nuhs/startscan.mp4" />
      <p className="padding-top-16">
        <b>Dead zones.</b> Collapsed menu appears 30–34cm in front of the user.
        The convention is 45cm for direct manipulation, but we found that a
        shorter distance makes the menu more intuitively reachable, especially
        as more operations are layered in the user's view.
      </p>
    </div>
    <div className="col-md-4 padding-left-24">
      <p className="lesson-header">Longitudinal axis</p>
      <LoopingVideo src="/images/work/nuhs/draggable.mp4" />
      <p className="padding-top-16">
        <b>Rotation.</b>
        The menu is offset to the left by default, so the center view is unobstructed.
        Clinicians can change where the menu is locked in their field of view to
        suit their unique workflows and preferences.
      </p>
    </div>
  </div>
</div>

<div className="container-fluid margin-top-36">
  <div className="row d-flex">
    <h2 id="instrument">Scalable system</h2>
    <div className="col-md-4 padding-right-24">
      <p className="lesson-header">Recalibration</p>
      <p>
        Our goal was to create the foundation for real-time object tracking and
        visualization. We implemented a flexible system to recognize multiple
        instruments and use cases.
      </p>
      <p className="text-black-50 padding-top-16">
        End-to-end task flow for calibrating device
      </p>
      <ol>
        <li>
          <b>Start screen.</b> Holographic plane with guiding lines to place
          device appears.
        </li>
        <li>
          <b>Additional instructions.</b> Once HL2 detects marker in view,
          prompt user with additional instructions.
        </li>
        <li>
          <b>Countdown.</b> When the marker is still, the countdown timer
          appears so confirm orientation.
        </li>
        <li>
          <b>System confirmation.</b> The HL2 registers the marker's orientation
          and user can confirm or restart calibration.
        </li>
      </ol>
    </div>
    <div className="col-md-8">
      <LoopingVideo src="/images/work/nuhs/calibrate-panel.mp4" />
    </div>
  </div>
</div>

<div className="container-fluid margin-top-36">
  <div className="row d-flex">
    <div className="col-md-4 padding-right-16 padding-bottom-24">
      <MarkdownImage
        src="/images/work/nuhs/holotwin-mock.png"
        width="1096px"
        height="616.5px"
        layout="fill"
      />
      <figcaption>
        In procedures, the needle will be occluded beneath the skin. The virtual
        instrument will be rendered in occluded part of the patient, using
        holographic guidance.
      </figcaption>
    </div>
  </div>
</div>{" "}

<PostLayout>

## Future

- Check out the HoloLens2 design system
- How to prototype

</PostLayout>
